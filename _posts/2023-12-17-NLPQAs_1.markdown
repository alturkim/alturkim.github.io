---
layout: post
title:  "NLP Q&As Series: Part 1: Language Models and Transformer.
"
date:   2023-12-17 15:39:08 +0300
categories: blogpost
---

### Links
This is a series of short questions and answers about NLP, originally shared on my LinkedIn page.
---

## Q1: What is a Language Model?
A language model serves as a probabilistic representation of one or more languages, determining the likelihood of a given sequence of words belonging to a particular language. Essentially, it answers the question: 

What is the probability of this word sequence being part of the language?
Consider two statements:
1. "Mohammed studied business in college."
2. "college Mohammed business in studied."
Statement 1 has a higher probability of being a valid English sentence compared to Statement 2.

Language models have evolved from statistical models, such as n-gram language models, to neural language models based on various types of recurrent neural networks. Presently, they predominantly rely on Transformers and are often referred to as Large Language Models (LLM).

Applications of language models are diverse, but a common thread is that any application generating a text sequence benefits from an LM. The LM guides the generation process by producing high-probability sequences. Examples of such applications include speech recognition, machine translation, and optical character/handwriting recognition.

<p align="center">
  <img alt="LM1" src="/assets/imgs/nlpqa1_1.png">
  <br>
    <em><a href="https://arxiv.org/abs/2303.18223">Source: A Survey of Large Language Models.</a></em>
</p>

<p align="center">
  <img alt="LM2" src="/assets/imgs/nlpqa1_2.png">
  <br>
    <em><a href="https://arxiv.org/abs/2303.18223">Source: A Survey of Large Language Models.</a></em>
</p>

## ğğŸ: ğƒğğ¬ğœğ«ğ¢ğ›ğ ğ­ğ¡ğ ğ“ğ«ğšğ§ğ¬ğŸğ¨ğ«ğ¦ğğ« ğ€ğ«ğœğ¡ğ¢ğ­ğğœğ­ğ®ğ«ğ ğ¢ğ§ ğš ğ¡ğ¢ğ ğ¡ ğ¥ğğ¯ğğ¥.

The Transformer model is a seq2seq, i.e. it processes input sequences and generates output sequences. 

The journey of a sentence in the Transformer involves two main components: the Encoder and the Decoder.

ğÌ²ğ«Ì²ğÌ²ğ©Ì²ğ«Ì²ğ¨Ì²ğœÌ²ğÌ²ğ¬Ì²ğ¬Ì²ğ¢Ì²ğ§Ì²ğ Ì²â€‚Ì²ğ’Ì²ğ­Ì²ğÌ²ğ©Ì²ğ¬Ì²:Ì²
 -ğ“ğ¨ğ¤ğğ§ğ¢ğ³ğšğ­ğ¢ğ¨ğ§: The sentence is divided into tokens.
 -ğ„ğ¦ğ›ğğğğ¢ğ§ğ ğ¬: Each token is transformed into a high-dimensional vector.
 -ğğ¨ğ¬ğ¢ğ­ğ¢ğ¨ğ§ğšğ¥ ğ„ğ§ğœğ¨ğğ¢ğ§ğ : Information about the token's position in the sentence is added to its embeddings.
_______________________________________________________________
ğ„Ì²ğ§Ì²ğœÌ²ğ¨Ì²ğÌ²ğÌ²ğ«Ì²:Ì²
The Encoder consists of multiple (e.g., 6) layers, each mainly having:

 -ğŒğ®ğ¥ğ­ğ¢-ğ¡ğğšğ ğ€ğ­ğ­ğğ§ğ­ğ¢ğ¨ğ§: Tokensâ€™ embeddings are passed to a neural network that updates them considering information from other tokens in the input.

 -ğ…ğğğ-ğ…ğ¨ğ«ğ°ğšğ«ğ: A neural network applied to each tokenâ€™s embeddings separately. 

_______________________________________________________________
ğƒÌ²ğÌ²ğœÌ²ğ¨Ì²ğÌ²ğÌ²ğ«Ì²:Ì²
The Decoder takes two inputs: the Encoderâ€™s output and previously generated tokens (as embeddings). The model generates the output sequence one token at a time.

A Decoder also consists of multiple layers, each mainly having:
 -ğŒğšğ¬ğ¤ğğ ğŒğ®ğ¥ğ­ğ¢-ğ¡ğğšğ ğ€ğ­ğ­ğğ§ğ­ğ¢ğ¨ğ§: For each generated token, updates its embeddings considering information from other tokens.

 -ğŒğ®ğ¥ğ­ğ¢-ğ¡ğğšğ ğ€ğ­ğ­ğğ§ğ­ğ¢ğ¨ğ§: For each generated token, updates its embeddings considering information from Encoder output.

 -ğ…ğğğ-ğ…ğ¨ğ«ğ°ğšğ«ğ: Utilizes a similar structure as in the Encoder.

_______________________________________________________________
ğ…ğ¢ğ§ğšğ¥ğ¥ğ²: The current output token is determined using a Linear Layer followed by a Softmax.

Note: This is a high-level description, some parts are left out, and some are oversimplified.

<p align="center">
  <img alt="Transformer" src="/assets/imgs/nlpqa2.png">
  <br>
    <em><a href="https://arxiv.org/abs/1706.03762">Source: Attention is All You Need.</a></em>
</p>